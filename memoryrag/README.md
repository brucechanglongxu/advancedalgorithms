# Hallucination

Extrinsic hallucinations in large language models (LLMs) refer to model outputs that are not grounded in the input context or real-world knowledge, typically memorized from pretraining data. These hallucinations occur when the model generates fabricated or inaccurate content that appears plausible. One root cause lies in the limitations of the pretraining corpusâ€”massive, uncurated internet data that can be outdated or incorrect. Fine-tuning exacerbates this by introducing new or conflicting information in small quantities, often leading to increased hallucinations, especially when models struggle to integrate this new knowledge. Research from Gekhman et al. (2024) shows that while LLMs can eventually learn unknown facts via fine-tuning, doing so increases hallucination risk and slows learning.

A growing body of work has developed tools and benchmarks to detect hallucinations. Techniques like FactualityPrompt, FActScore, SAFE, and FacTool quantify factuality by comparing model outputs against Wikipedia or web search evidence. Some methods, like SelfCheckGPT, work without external references by checking consistency across multiple model generations. Meanwhile, datasets like TruthfulQA and SelfAware test whether models know when they don't know something, revealing that models often fail adversarial or unanswerable questions. Calibration studies suggest that while LLMs can estimate their own uncertainty fairly well, reinforcement learning with human feedback (RLHF) often degrades this calibration.

To reduce hallucinations, new approaches combine retrieval with generation. Methods like RARR, FAVA, Self-RAG, and CoVe incorporate search results to revise or critique generations. These retrieval-augmented strategies are more effective at grounding outputs, especially for long-form or fact-dense tasks. Others like RECITE introduce intermediate "recitation" steps to prompt model memory retrieval before answering. In parallel, inference-time interventions and new sampling methods like factual-nucleus sampling enhance factuality by tweaking model activations or token selection strategies. Finally, fine-tuning methods such as FLAME and FactTune explicitly train models for factual precision using automated claim verification and preference datasets, aiming to align output with verifiable knowledge while preserving useful, truthful content.

