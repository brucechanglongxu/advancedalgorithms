**[Compressed SimCLR and BYOL with Conditional Entropy Bottleneck](https://discuss.ai.google.dev/t/google-research-self-supervised-learning-compressed-simclr-byol-with-conditional-entropy-bottleneck-with-tensorflow-code/23763):** Recent contrastive approaches to self-supervised visual representation learning aim to learn representations that maximally capture the mutual information between two transformed views of an image... the primary idea of these approaches is that this mutual information corresponds to a general shared context that is invariant to various transformaitons of the input, and it is assumed that such invariant features will be effective for various downstream higher-level tasks. However, although existing contrastive approaches maximize mutual information between augmented views of the same input, they do not necessarily compress away _irrelevant information_ from these views - retaining irrelevant information often leads to less stable representations and to failures in robustness and generalization, which hinders the efficacy of learned representations. 

BYOL solves this redundancy issue which uses a slow-moving average network to learn consistent, view-invariant representations of the inputs. However it does not explicitly capture relevant compression in its objective. The above work modifies SimCLR by adding information compression using _Conditional Entropy Bottleneck (CEB)_ and also does the same for BYOL. CEB enables measurement and control of information compression in learned representations, and its subsequent impact on downstream tasks. 

# Contrastive Learning

Much recent progress in AI has come from techniques that do not require labeled data but instead create self-supervised learning objectives. Two major families are contrastive learning and masked prediction strategies. These approaches enable learning rich representations from unlabeled data, which can then be fine-tuned for downstream tasks â€“ crucial for leveraging the vast amounts of raw data available. Contrastive learning is about learning an embedding space in which similar inputs are pulled together and dissimilar inputs are pushed apart. In computer vision, _"similar"_ often means two different augmentations of the same image. 

Both SimCLR and MoCo follow the principal of _instance discrimination_ i.e. treat each image as its own class. This learns general features, but one issue is reliance on negative sampling. It was believed that we needed negatives to prevent collapse (trivial solution where all embeddings are identical), however new approaches challenge this assumption. 

## SimCLR

A seminal example is **SimCLR (Simple Contrastive Learning of Representations)** by Google Brain. In SimCLR, for each image, two augmented views are generated (random crops, color jitter, flip, blur etc.) forming a positive pair. A deep network (CNN or ViT) encodes each view into a representation vector, then a small projection MLP maps it to a latent space where the contrastive loss is applied. The training objective (NT-Xent loss, a normalized temperature-scaled cross-entropy) is to make the representation of image's two views close, while pushing representations of different images apart. In practice, given a batch of N images (which produces 2N augmentation), each augmented pair contributes a _"positive"_ match, and there are 2N-2 other images' augmentations treated as negatives. This encourages the model to **learn features invariant to the chosen augmentations** (since an image and its distorted version should map to similar embeddings) and discriminative enough to distinguish different images. 

_Large batch size_ is important for SimCLR because the more negative examples in the batch, the better it learns to carve the feature space. SimCLR demonstrated that with heavy data augmentation and big batches (e.g. batch size 4096), a model could learn image features almost as good as supervised learning. Notably, it required no labels - the training signal comes from the instance discrimination task (each image is its own class against others). 

## MoCo

Published by Facebook AI, MoCo maintains a _memory bank/queue_ of representations from recent batches and a momentum updated encoder for consistency. Instead of relying on a huge batch for negatives, MoCo builds a dictionary of keys (stored feature vectors) that serve as negative samples, and this dictionary is continuously updated. It has two networks: a query encoder (current network) and a key encoder (momentum version). For a given image, the query encoder encodes the augmented image, the key encoder (which lags behind, updated slowly) encodes another augmentaiton of that same image to get the positive key, and the queue provides a set of previous keys as negatives. The loss is again contrastive (InfoNCE): make the query close to its positive key, far from all others in the queue. The **momentum encoder** is updated as:

$$\theta_{key} < m * \theta_{key} + (1 - m) * \theta_{query}$$

with $$m ~ 0.999$$. This slow update ensures that the keys in the queue change slowly, providing a more stable target even as the query encoder changes rapidly. The upshot is that MoCo can use a consistent large pool of negatives without needing a giant batch or huge memory - the queue might hold say 65k recent examples. MoCo showed one can effectively do contrastive learning in an _asymmetric_ setup (two networks) and was state of the art for a period of time -- inspiring variants like MoCo v2/v3, which incorporate improvements (stronger augmentation, MLP head, ViT backbone in v3). 
