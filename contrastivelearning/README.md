# Contrastive Learning

Much recent progress in AI has come from techniques that do not require labeled data but instead create self-supervised learning objectives. Two major families are contrastive learning and masked prediction strategies. These approaches enable learning rich representations from unlabeled data, which can then be fine-tuned for downstream tasks â€“ crucial for leveraging the vast amounts of raw data available.

Contrastive learning is about learning an embedding space in which similar inputs are pulled together and dissimilar inputs are pushed apart. In computer vision, _"similar"_ often means two different augmentations of the same image. A seminal example is **SimCLR (Simple Contrastive Learning of Representations)** by Google Brain. In SimCLR, for each image, two augmented views are generated (random crops, color jitter, flip, blur etc.) forming a positive pair. A deep network (CNN or ViT) encodes each view into a representation vector, then a small projection MLP maps it to a latent space where the contrastive loss is applied. The training objective (NT-Xent loss, a normalized temperature-scaled cross-entropy) is to make the representation of image's two views close, while pushing representations of different images apart. In practice, given a batch of N images (which produces 2N augmentation), each augmented pair contributes a _"positive"_ match, and there are 2N-2 other images' augmentations treated as negatives. This encourages the model to **learn features invariant to the chosen augmentations** (since an image and its distorted version should map to similar embeddings) and discriminative enough to distinguish different images. 

_Large batch size_ is important for SimCLR because the more negative examples in the batch, the better it learns to carve the feature space. SimCLR demonstrated that with heavy data augmentation and big batches (e.g. batch size 4096), a model could learn image features almost as good as supervised learning. Notably, it required no labels - the training signal comes from the instance discrimination task (each image is its own class against others). 
