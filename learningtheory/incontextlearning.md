# In Context Learning

In-Context Learning (ICL) is the ability of large language models (LLMs) to learn new tasks at inference time by conditioning on examples provided in the input prompt, without updating any model weights. It allows LLMs to generalize to unseen tasks and domains purely through pattern recognition and attention over tokens. ICL is foundational to how models like GPT-3/4, ChatGPT, Claude, and LLaMA operate. It underpins behaviors such as few-shot learning, zero-shot generalization, instruction-following, and even tool-use â€” all via prompt design.