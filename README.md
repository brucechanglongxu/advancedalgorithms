## Distributed Training and Scalability

Training modern AI models, especially large deep networks like LLMs or huge vision models, often requires distributing the computation across multiple GPUs or even multiple machines. Distributed training is essential to handle _very large_ models (that don't fit in one GPU's memory) and to speed up training on _large datasets_ by parallelizing workload. There are several parallelism paradigms, each addressing different bottlenecks. 

### Parallelism Strategies: Data, Model and Pipeline
**Data Parallelism (DP)** is the most common approach. In data parallelism, we replicate the entire model on each of the N parallel workers (GPUs) and each worker processes a different subset of the data batch. For example, with 4 GPUs, each GPU might get 1/4 of the images in a minibatch; they perform a forward and backward pass on their data and compute gradients locally. Then a synchrnoization step (all-reduce operation) averages gradients across GPUs, and these averaged gradients are used to update the model weights so that all replicas stay in sync. In effect, data parallelism increases the effective batch size and accelerates training almost linealry with the numbe rof workers (minus communication overhead). Frameworks like PyTorch's **DistributedDataParallel** or **Horovod** implement this. The challenge is the communication cost: gradients (and sometimes model parameters) must be exchanged each step. Efficient all-reduce algorithms and high-bandwidth interconnects (InfiniBand, NVLink) are critical to scale. 

In summary, the primary reason is to scale training efficiently across GPUs, and alleviates memory constraints by splitting the data batch; we run the same model on all devices, with different data shards and synchronized gradients. Suppose we have a batch size of 512 and ResNet-50, on 1-GPU this might be out of memory, but on 4 GPUs with data parallelism, each batch will have a size of 128, which makes it easier to fit on each GPU. So we would use this when our model is small but we want to train on massive batches. If our model is too big to fit on one GPU, we would use model parallelism or pipeline parallelism. 

**Model Parallelism (MP):** splits the model's parameters across multiple devices, where each GPU holds a differet part of the model. The first is _layer-wise_ model parallelism, where if a network has 12 layers and 3 GPUs, we put 4 layers on each GPU. During a forward pass, GPU1 processes layers 1-4, then passes intermediate activations to GPU2 for layers 5-8, and then GPU3 for layers 9-12. The backward pass goes in reverse. This is quite "naive" however and can lead to idle time as GPU2 needs to wait for GPU1 to finish. 

### Mixed Precision and Efficient Training

Another key to scaling training is using lower numerical precision. _Mixed precision training_ combines 16-bit floating point (FP16 and BFloat16) with 32-bit (FP32) in a way that doesn't degrade model quality [See NVIDIA blog]. Neural networks don't always need 32-bit precision for every calculation - using FP16 halves memory per value and can significantly speed up tensor operations on modern GPUs (which have specialized hardware, Tensor Cores, for low-precision matrix math). Mixed precision typically keys a master copy of weights in FP32 to accumulate small gradient updates accurately, but does forward and backward computations in FP16. 
