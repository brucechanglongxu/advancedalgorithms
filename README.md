## Distributed Training and Scalability

Training modern AI models, especially large deep networks like LLMs or huge vision models, often requires distributing the computation across multiple GPUs or even multiple machines. Distributed training is essential to handle _very large_ models (that don't fit in one GPU's memory) and to speed up training on _large datasets_ by parallelizing workload. There are several parallelism paradigms, each addressing different bottlenecks. 

### Parallelism Strategies: Data, Model and Pipeline
**Data Parallelism (DP)** is the most common approach. In data parallelism, we replicate the entire model on each of the N parallel workers (GPUs) and each worker processes a different subset of the data batch. For example, with 4 GPUs, each GPU might get 1/4 of the images in a minibatch; they perform a forward and backward pass on their data and compute gradients locally. Then a synchrnoization step (all-reduce operation) averages gradients across GPUs, and these averaged gradients are used to update the model weights so that all replicas stay in sync. In effect, data parallelism increases the effective batch size and accelerates training almost linealry with the numbe rof workers (minus communication overhead). Frameworks like PyTorch's **DistributedDataParallel** or **Horovod** implement this. The challenge is the communication cost: gradients (and sometimes model parameters) must be exchanged each step. Efficient all-reduce algorithms and high-bandwidth interconnects (InfiniBand, NVLink) are critical to scale. 

In summary, the primary reason is to scale training efficiently across GPUs, and alleviates memory constraints by splitting the data batch; we run the same model on all devices, with different data shards and synchronized gradients. Suppose we have a batch size of 512 and ResNet-50, on 1-GPU this might be out of memory, but on 4 GPUs with data parallelism, each batch will have a size of 128, which makes it easier to fit on each GPU. So we would use this when our model is small but we want to train on massive batches. If our model is too big to fit on one GPU, we would use model parallelism or pipeline parallelism. 

**Model Parallelism (MP):** splits the model's parameters across multiple devices, where each GPU holds a differet part of the model. The first is _layer-wise_ model parallelism, where if a network has 12 layers and 3 GPUs, we put 4 layers on each GPU. During a forward pass, GPU1 processes layers 1-4, then passes intermediate activations to GPU2 for layers 5-8, and then GPU3 for layers 9-12. The backward pass goes in reverse. This is quite "naive" however and can lead to idle time as GPU2 needs to wait for GPU1 to finish. _Pipeline parallelism_ addresses this idletime issue by splitting the minibatch into micro-batches and pipelining them through the model chain so all GPUs can work in parallel on different micro-batches. For example, while GPU2 is processing micro-batch 1 through layers 5-8, GPU1 can start processing micro-batch 2 through layers 1-4, similar to an assembly line. _GPipe_ and _PipeDream_ implement pipeline parallelism, which improves utilization at the cost of some latency (and introduces issues like pipeline flush at the end of an epoch). Pipeline parallelism overlaps computation across model partitions. 

**Tensor Parallelism (TP):** Horizontal model parallelism splits computation within a layer, for example in a Transformer, the large matrix multiplications for attention / feed-forward can be split: if a weight matrix is in size, two GPUs can each hold half of the rows (or columns) of the matrix and each compute their part of the output, then concatenate the results. This way, a single layer's arithmetic is shared. Libraries like Megatron-LM use tensor parallelism to distribute the dot products and reduce memory per GPU. Combining tensor parallelism with pipeline parallelism is common in very large LLM training (e.g. splitting each Transformer layer across GPUs and also stacking layers on different GPUs). 

In practice, to train trillion-parameter models, we need a combination of data, model and pipeline parallelism simultaneously. For instance, we can pipeline across nodes, tensor-split each layer across GPUs within a node, and do data parallel updates across multiple groups. 

### Mixed Precision and Efficient Training

Another key to scaling training is using lower numerical precision. _Mixed precision training_ combines 16-bit floating point (FP16 and BFloat16) with 32-bit (FP32) in a way that doesn't degrade model quality [See NVIDIA blog]. Neural networks don't always need 32-bit precision for every calculation - using FP16 halves memory per value and can significantly speed up tensor operations on modern GPUs (which have specialized hardware, Tensor Cores, for low-precision matrix math). Mixed precision typically keys a master copy of weights in FP32 to accumulate small gradient updates accurately, but does forward and backward computations in FP16. 
