# vLLM

vLLM is a fast, memory-efficient LLM inference engine built on top of PyTorch and CUDA. It enables high-throughput, low-latency serving of LLMs like LLaMA, Mistral, Falcon, GPT-J, and others.

